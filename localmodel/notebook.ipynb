{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log1\n"
     ]
    }
   ],
   "source": [
    "print(\"log1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.12)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.84)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.2.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.2.12)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.1.84)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (1.10.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docarray in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.40.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (1.26.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (3.10.6)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (1.10.8)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (2.32.0.20240622)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.10.8->docarray) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.1.0->docarray) (2.18.0)\n",
      "Requirement already satisfied: urllib3>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from types-requests>=2.28.11.6->docarray) (2.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic==1.10.8 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.10.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic==1.10.8) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadbNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.10.8)\n",
      "Collecting chroma-hnswlib==0.7.5 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.5-cp312-cp312-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.18.1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.64.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.1.3-cp39-abi3-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-4.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.32.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.27.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting setuptools>=16.0 (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading setuptools-70.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.1-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-0.22.0-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-12.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading chromadb-0.5.4-py3-none-any.whl (581 kB)\n",
      "   ---------------------------------------- 0.0/581.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 41.0/581.4 kB 653.6 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 71.7/581.4 kB 787.7 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 194.6/581.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 256.0/581.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 327.7/581.4 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 409.6/581.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 471.0/581.4 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 553.0/581.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  573.4/581.4 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- 581.4/581.4 kB 589.5 kB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.5-cp312-cp312-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/152.0 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 81.9/152.0 kB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 143.4/152.0 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 152.0/152.0 kB 259.1 kB/s eta 0:00:00\n",
      "Using cached bcrypt-4.1.3-cp39-abi3-win_amd64.whl (158 kB)\n",
      "Using cached build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached grpcio-1.64.1-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "Using cached kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached mmh3-4.1.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Using cached onnxruntime-1.18.1-cp312-cp312-win_amd64.whl (5.6 MB)\n",
      "Using cached opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "Using cached opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "Using cached posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_auth-2.32.0-py2.py3-none-any.whl (195 kB)\n",
      "   ---------------------------------------- 0.0/195.5 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 143.4/195.5 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/195.5 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/195.5 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/195.5 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/195.5 kB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 195.5/195.5 kB 741.0 kB/s eta 0:00:00\n",
      "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached httptools-0.6.1-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/413.4 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 51.2/413.4 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- 413.4/413.4 kB 573.5 kB/s eta 0:00:00\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.2/42.2 kB 46.5 kB/s eta 0:00:00\n",
      "Using cached watchfiles-0.22.0-cp312-none-win_amd64.whl (280 kB)\n",
      "Using cached websockets-12.0-cp312-cp312-win_amd64.whl (124 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB 5.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.4/6.2 MB 4.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/6.2 MB 4.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/6.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.9/6.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.1/6.2 MB 3.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 3.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.5/6.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.2 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.9/6.2 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.0/6.2 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.2/6.2 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.7/6.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.0/6.2 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.5/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.4/6.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.4/6.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.6/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.1/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.5 MB/s eta 0:00:00\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   --------------------------------------  174.1/177.6 kB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  174.1/177.6 kB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  174.1/177.6 kB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  174.1/177.6 kB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  174.1/177.6 kB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 177.6/177.6 kB 563.4 kB/s eta 0:00:00\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 460.8/536.2 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- 536.2/536.2 kB 701.3 kB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.2 kB ? eta -:--:--\n",
      "   ------------------------------------- - 174.1/181.2 kB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 174.1/181.2 kB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 174.1/181.2 kB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- 181.2/181.2 kB 994.1 kB/s eta 0:00:00\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading setuptools-70.3.0-py3-none-any.whl (931 kB)\n",
      "   ---------------------------------------- 0.0/931.1 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 297.0/931.1 kB 6.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 481.3/931.1 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 522.2/931.1 kB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 522.2/931.1 kB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 655.4/931.1 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/931.1 kB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- 931.1/931.1 kB 982.7 kB/s eta 0:00:00\n",
      "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 81.9/85.3 kB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 81.9/85.3 kB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 81.9/85.3 kB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 81.9/85.3 kB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 81.9/85.3 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 85.3/85.3 kB 320.8 kB/s eta 0:00:00\n",
      "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Installing collected packages: pyreadline3, pypika, mpmath, monotonic, mmh3, flatbuffers, zipp, wrapt, websockets, ujson, sympy, shellingham, setuptools, python-multipart, python-dotenv, pyproject_hooks, pyasn1, protobuf, opentelemetry-util-http, oauthlib, importlib-resources, humanfriendly, httptools, grpcio, fsspec, dnspython, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, importlib-metadata, huggingface-hub, googleapis-common-protos, email_validator, deprecated, coloredlogs, build, typer, tokenizers, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, opentelemetry-semantic-conventions, opentelemetry-instrumentation, kubernetes, fastapi-cli, opentelemetry-sdk, opentelemetry-instrumentation-asgi, fastapi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 build-1.2.1 cachetools-5.3.3 chroma-hnswlib-0.7.5 chromadb-0.5.4 coloredlogs-15.0.1 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 flatbuffers-24.3.25 fsspec-2024.6.1 google-auth-2.32.0 googleapis-common-protos-1.63.2 grpcio-1.64.1 httptools-0.6.1 huggingface-hub-0.23.4 humanfriendly-10.0 importlib-metadata-7.1.0 importlib-resources-6.4.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.18.1 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 posthog-3.5.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pypika-0.48.9 pyproject_hooks-1.1.0 pyreadline3-3.4.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-oauthlib-2.0.0 rsa-4.9 setuptools-70.3.0 shellingham-1.5.4 starlette-0.37.2 sympy-1.13.0 tokenizers-0.19.1 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 watchfiles-0.22.0 websockets-12.0 wrapt-1.16.0 zipp-3.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.10.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from qdrant-client) (1.64.1)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio_tools-1.64.1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.26 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from qdrant-client) (1.26.4)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from qdrant-client) (1.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from qdrant-client) (2.2.1)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.27.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (70.3.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joshita.gautam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (4.12.2)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading qdrant_client-1.10.1-py3-none-any.whl (254 kB)\n",
      "   ---------------------------------------- 0.0/254.1 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/254.1 kB 1.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 61.4/254.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 163.8/254.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 204.8/254.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 245.8/254.1 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 245.8/254.1 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- 254.1/254.1 kB 781.6 kB/s eta 0:00:00\n",
      "Downloading grpcio_tools-1.64.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.1/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.2/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.2/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.4/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.4/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.4/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.1 MB 972.0 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.5/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.5/1.1 MB 1.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.6/1.1 MB 965.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.6/1.1 MB 926.6 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.6/1.1 MB 899.0 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.6/1.1 MB 888.5 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.1 MB 839.1 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.1 MB 831.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.1 MB 807.5 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.7/1.1 MB 786.0 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.8/1.1 MB 779.7 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 762.9 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.1 MB 756.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 752.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 749.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 749.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.0/1.1 MB 743.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.0/1.1 MB 722.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.0/1.1 MB 713.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.0/1.1 MB 704.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 696.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 695.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 695.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 652.0 kB/s eta 0:00:00\n",
      "Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
      "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/57.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 30.7/57.5 kB 435.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 51.2/57.5 kB 372.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 51.2/57.5 kB 372.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 57.5/57.5 kB 215.7 kB/s eta 0:00:00\n",
      "Downloading protobuf-5.27.2-cp310-abi3-win_amd64.whl (426 kB)\n",
      "   ---------------------------------------- 0.0/426.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/426.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/426.9 kB 660.6 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 61.4/426.9 kB 469.7 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 81.9/426.9 kB 459.5 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 92.2/426.9 kB 438.1 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 122.9/426.9 kB 425.1 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 143.4/426.9 kB 449.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 163.8/426.9 kB 468.3 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 184.3/426.9 kB 464.2 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 184.3/426.9 kB 464.2 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 215.0/426.9 kB 437.6 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 235.5/426.9 kB 437.4 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 266.2/426.9 kB 431.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/426.9 kB 416.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/426.9 kB 416.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/426.9 kB 416.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/426.9 kB 416.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 348.2/426.9 kB 416.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 389.1/426.9 kB 433.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 399.4/426.9 kB 422.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  419.8/426.9 kB 429.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 426.9/426.9 kB 342.1 kB/s eta 0:00:00\n",
      "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: protobuf, portalocker, hyperframe, hpack, h2, grpcio-tools, qdrant-client\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed grpcio-tools-1.64.1 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 portalocker-2.10.0 protobuf-5.27.2 qdrant-client-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.25.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.27.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install qdrant-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "MODEL='llama2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSure! Here's a short and simple one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "model=Ollama(model=MODEL)\n",
    "embeddings=OllamaEmbeddings()\n",
    "model.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSure, here's another one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser= StrOutputParser()\n",
    "chain= model| parser\n",
    "chain.invoke(\"tell me another joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rizve2021.pdf', 'page': 0}, page_content='Gabriella: An Online System for Real-Time Activity\\nDetection in Untrimmed Security Videos\\nMamshad Nayeem Rizve∗, Ugur Demir∗, Praveen Tirupattur∗, Aayush Jung Rana∗,\\nKevin Duarte∗, Ishan R Dave∗, Yogesh S Rawat†and Mubarak Shah†\\nCenter for Research in Computer Vision\\nUniversity of Central Florida, Orlando, Florida, USA\\nEmail:∗[nayeemrizve, ugur, praveentirupattur, aayushjr, kevin, ishandave]@knights.ucf.edu,†[yogesh, shah]@crcv.ucf.edu\\nAbstract —Activity detection in security videos is a difﬁcult\\nproblem due to multiple factors such as large ﬁeld of view,\\npresence of multiple activities, varying scales and viewpoints, and\\nits untrimmed nature. The existing research in activity detection is\\nmainly focused on datasets, such as UCF-101, JHMDB, THUMOS,\\nand A V A, which partially address these issues. The requirement\\nof processing security videos in real-time makes this even more\\nchallenging. In this work, we propose Gabriella, a real-time\\nonline system to perform activity detection on untrimmed security\\nvideos. The proposed method consists of three stages: tubelet\\nextraction, activity classiﬁcation, and online tubelet merging. For\\ntubelet extraction, we propose a localization network which takes\\na video clip as input and spatio-temporally detects potential\\nforeground regions at multiple scales to generate action tubelets.\\nWe propose a novel Patch-Dice loss to handle large variations\\nin actor size. Our online processing of videos at a clip level\\ndrastically reduces the computation time in detecting activities.\\nThe detected tubelets are assigned activity class scores by the\\nclassiﬁcation network and merged together using our proposed\\nTubelet-Merge Action-Split (TMAS) algorithm to form the ﬁnal\\naction detections. The TMAS algorithm efﬁciently connects the\\ntubelets in an online fashion to generate action detections which\\nare robust against varying length activities. We perform our\\nexperiments on the VIRAT and MEV A (Multiview Extended Video\\nwith Activities) datasets and demonstrate the effectiveness of the\\nproposed approach in terms of speed ( ∼100 fps) and performance\\nwith state-of-the-art results. More details about this work are\\navailable on our project webpage1.\\nI. I NTRODUCTION\\nDeep convolutional neural networks have achieved impres-\\nsive action classiﬁcation results in recent years [3], [31],\\n[32]. Similar advancements have been made for the tasks of\\naction detection in trimmed videos [6], [15], [29], where the\\nspatial extents of the actions are estimated, and temporal action\\nlocalization in untrimmed videos [23], [36], where only the\\ntemporal extents of the activities are predicted. However, these\\nimprovements have not been transferred to action detection in\\nuntrimmed videos, where both the spatial and temporal extents\\nof the activities must be found; current approaches have yet to\\nachieve high performance on this difﬁcult task.\\nAction detection in untrimmed videos has several major\\nchallenges: multiple activity types may occur simultaneously,\\nmultiple actors may be present, and the temporal extents of\\nthe activities are unknown. Videos in trimmed action detection\\n1https://www.crcv.ucf.edu/research/projects/gabriella-an-online-system-for-\\nreal-time-activity-detection-in-untrimmed-security-videos\\nFig. 1. Top: Two Sample frames from different scenes of the VIRAT dataset\\nshowing variation in perspective, scale, and ﬁeld-of-view. Bottom : Sample\\nframes from the A V A [11] (left) and THUMOS’14 [14] (right) dataset. The\\nVIRAT dataset contains a greater number of concurrent actions as well as a\\ngreater variety of action scales (both spatially and temporally).\\ndatasets, like A V A [11], contain multiple actors and activ-\\nities simultaneously, but each video is trimmed into three\\nsecond clips which have bounding-box annotations only on\\nthe center frame. Untrimmed action detection datasets, like\\nTHUMOS’14 [14], are comprised of untrimmed videos, but\\neach video contains only one or two actors performing the'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 0}, page_content='the center frame. Untrimmed action detection datasets, like\\nTHUMOS’14 [14], are comprised of untrimmed videos, but\\neach video contains only one or two actors performing the\\nsame action. Although difﬁcult, these datasets do not contain\\nall the aforementioned challenges, which results in action\\ndetection methods that perform poorly when evaluated on\\nvideos containing all these challenges. Therefore, in this work\\nwe focus on the security video datasets: VIRAT [21] and\\nMEV A (Multiview Extended Video with Activities) [1]. Not\\nonly do these two action detection datasets have untrimmed\\nvideos with multiple activity types and multiple actors, but\\nalso they are comprised of multiple viewpoints and contain\\nseveral actors performing multiple actions concurrently . These\\nactors have varying scales and actor sizes tend to be extremely\\nsmall relative to the video frame, which makes the detection of\\nactivities in these datasets extremely difﬁcult. Figure 1 shows\\nsample frames from the VIRAT dataset and compares them\\nwith frames from the THUMOS’14 and A V A datasets.\\nWe focus on untrimmed security videos and propose\\nGabriella , an online real-time system for action detection. Our2020 25th International Conference on Pattern Recognition (ICPR)\\nMilan, Italy, Jan 10-15, 2021\\n978-1-7281-8808-9/20/$31.00 ©2020 IEEE 42372020 25th International Conference on Pattern Recognition (ICPR) | 978-1-7281-8808-9/21/$31.00 ©2021 IEEE | DOI: 10.1109/ICPR48806.2021.9412791\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 1}, page_content='method is composed of three modules: tubelet localization,\\ntubelet classiﬁcation, and tubelet merging. Our action local-\\nization module generates pixel-level foreground-background\\nsegmentations which localize actions in short video clips. These\\npixel-level localizations are turned into short spatio-temporal\\naction tubelets, which are passed to a classiﬁcation network to\\nobtain multi-label predictions. After classiﬁcation, the tubelets\\nmust be linked together to obtain the ﬁnal detections with vary-\\ning length; to this end, our novel online Tubelet-Merge Action-\\nSplit (TMAS) algorithm merges these short action tubelets into\\nﬁnal action detections.\\nConventional action detection methods make use of pre-\\ntrained, frame-based object detectors to localize all potential\\nactors within the video. Frame-based object detection sys-\\ntems, [10], have two main issues: 1) processing each frame\\nindependently requires large amounts of computation, which\\nreduces the overall speed of the system and leads to temporally\\ninconsistent detections between adjacent frames, and 2) all\\nobjects within the frame are detected, even those which are not\\nperforming actions. Our action localization module processes\\nmultiple frames simultaneously and only produces tubelets\\nwhich correspond to possible actions within the video. This\\nresults in temporally consistent localizations as well as a re-\\nduction in the number of proposals, which drastically increases\\nthe speed of the overall system. To improve the accuracy of our\\nlocalization network, we propose a novel Patch-Dice loss. The\\noriginal global Dice loss [30] allows networks to account for\\nlarge imbalances between foreground and background (which\\nis the case for security videos with very small actors). However,\\nit does not take into account the variation in the scale of\\ndifferent foreground objects/actions, which leads networks to\\nfocus on only the largest actions. The Patch-Dice loss solves\\nthis, allowing our network to localize actions of any scale by\\ncomputing loss on local neighborhoods of each frame.\\nSince activities in untrimmed videos can vary in length, it is\\nnecessary to handle both short, atomic activities, like ‘opening\\na door’ or‘exiting a vehicle’ , as well as long, repetitive\\nactions like ‘walking’ or‘riding’ . To this end, our system\\nprocesses videos in an online fashion. Once the short tubelets\\nhave been localized and classiﬁed, our Tubelet-Merge Action-\\nSplit (TMAS) algorithm merges them into ﬁnal action tubes of\\nvarying lengths. By classifying short tubelets and merging them\\ninto action tubes, our system successfully detects both atomic\\nand repetitive actions. Also, since each tube can have multiple\\nactivities co-occurring simultaneously, the TMAS algorithm\\nsplits them to successfully isolate individual activities. Due\\nto the online nature of the TMAS algorithm, as well as the\\nefﬁciency of the localization network, our system generates\\naction detections at over 100 fps, greatly exceeding the speed\\nof contemporary action detection methods.\\nOur contributions are as follows:\\n•We introduce Gabriella , areal-time online system that\\nperforms action detection in untrimmed security videos at\\n∼100frames per second.\\n•We propose an action localization network, trained using a\\nnovel Patch-Dice loss , to detect activity agnostic tubeletsof varying scale which signiﬁcantly reduces the processing\\ntime of the system.\\n•The proposed TMAS tubelet merging algorithm efﬁciently\\nconnects the tubelets in an online fashion and produces\\ndetections that are consistent across time as well as robust\\nagainst varying length activities.\\nWe evaluate the proposed approach on the VIRAT and\\nMEV A datasets and obtain state-of-the-art results in terms of\\nboth speed and performance.\\nII. R ELATED WORKS\\nConvolutional Neural Networks (CNN) have been studied\\nfor video analysis and applied successfully for the action\\nrecognition problem [3], [31], [33]. Earlier approaches fuse'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 1}, page_content='II. R ELATED WORKS\\nConvolutional Neural Networks (CNN) have been studied\\nfor video analysis and applied successfully for the action\\nrecognition problem [3], [31], [33]. Earlier approaches fuse\\n2D frame features to extract temporal information [16], while\\nrecent works mostly apply 3D convolutions to extract spatio-\\ntemporal features simultaneously [3], [7], [31]. The works in\\n[7], [28] use two-stream network architectures to further exploit\\ntemporal dependencies.\\nMost action classiﬁers expect short trimmed videos, however,\\nthis is unrealistic for action recognition in real-world security\\nvideos. Predicting the temporal extents of actions is necessary\\nfor reliable recognition. In [23], a new layer is proposed to\\ntemporally localize activities in videos of the MultiThumos\\ndataset [36]. Most works on spatial action detection rely on\\na region proposal network [25] to detect multiple objects in\\neach frame and combine them temporally to generate action\\ntubelets [22], [35]. In [13], a 3D CNN network efﬁciently\\npredicts frame-wise background-foreground segmentation map\\nand extrapolates the action tubes. However, such approaches\\nbecome computationally inefﬁcient as the number of proposals\\ngrows larger, making it unsuitable for real time performance.\\nAction detection in untrimmed security videos requires to\\naddress multiple challenges. In [10], a frame-level object\\ndetection and optical ﬂow based model is proposed to solve\\naction detection on the VIRAT [21] dataset. They use hi-\\nerarchical clustering on all detected objects in a video to\\nobtain tube proposals and use optical ﬂow to perform action\\nclassiﬁcation. In [20] authors also perform frame-level object\\ndetection followed by tracking to generate proposals. These\\napproaches are computationally expensive and not suitable for\\nonline processing. Recent work by [9] improves upon [10]\\nto make the system real-time by reducing cluster points per\\nvideo, subsampling rate, and use GPU accelerated optical\\nﬂow computation. However, this approach produces too many\\nproposals which ends up affecting the system performance.\\nOur method uses a 3D CNN network for spatio-temporal ac-\\ntion segmentation which produces temporally consistent predic-\\ntions with fewer proposals. Additionally, our system processes\\nvideos in an online fashion without using computationally\\nexpensive methods (region proposal network, tracking, and\\noptical ﬂow) and achieves better performance in real-time.\\nIII. M ETHODOLOGY\\nA. Overview\\nAn overview of our system can be found in Figure 2.\\nEach untrimmed video is ﬁrst split into video clips, which4238\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 2}, page_content='Fig. 2. Overview of the proposed Gabriella system for action detection in untrimmed videos. An untrimmed video is processed clip by clip and fed into\\nthe localization network, producing localization masks. Tubelet extraction produces tubelets for each clip, which are then classiﬁed and passed to our TMAS\\nsystem. The classiﬁed tubelets are merged to create action-agnostic tubes, from which individual action-speciﬁc detections are obtained.\\nare each passed to a localization network to localize potential\\naction tubelets. Then, a network classiﬁes all possible action\\nclasses present within each tubelet. Finally, our TMAS system\\nsimultaneously ﬁlters and combines these short tubelets into\\nlonger action tubes. Since our system works on individual video\\nclips in an online fashion, it is able to produce these action\\ndetections in real-time. In this section, we will describe the\\ndifferent components of our proposed method.\\nB. Localization Network\\nThe localization network processes a short video clip and\\nlocalizes all actions within the clip. The network uses an\\nencoder-decoder structure which extracts class-agnostic action\\nfeatures and generates segmentation masks. We utilize a 3D\\nconvolution based encoder, I3D [3], to extract spatio-temporal\\nfeatures required for activity localization. The decoder must\\nuse these features to segment regions from the original input\\nwhich contain activities. Following recent works in image\\nsegmentation [4], [5], [19] and video segmentation [6], [13], we\\nuse a decoder structure which combines transpose convolutions\\nand upsampling. Stacking many transpose convolution layers\\nis computationally intensive, so we interleave upsampling\\noperations to interpolate features. This results in a shallow\\ndecoder network, which prevents over-parameterization and\\navoids overﬁtting.\\nTo obtain accurate action localizations, our decoder makes\\nuse of skip connections [27], feature pyramids [19], and atrous\\nconvolutions [4]. The decoders’ input features have been down-\\nsampled by the encoder, so to obtain ﬁne-grained segmentations\\nwe utilize skip connections from higher resolution layers of the\\nencoder. Since many of the actors within security videos varyin scale, the decoder makes use of feature pyramids: we stack\\nfeatures from various decoder layers (through upsampling) to\\nobtain feature representations at different scales. Furthermore,\\nwe apply atrous convolutions to the ﬁnal feature representation\\nof the decoder so that the decoder may learn the contextual\\ninformation necessary for action localization.\\nPatch-Dice Loss: The ﬁnal output of our localization net-\\nwork is a segmentation mask, ˆy, where each pixel is assigned\\na probability of being a part of an action. Given the ground-\\ntruth foreground/background mask, y, the network is trained\\nend-to-end using a sum of two losses. The ﬁrst is the binary\\ncross-entropy (BCE) loss,\\nLBCE(y,ˆy) =−1\\nNN∑\\ni=1[yilog(ˆyi) + (1−yi)log(1−ˆyi)], (1)\\ncomputed over all Npixels. Since the actors tend to be small in\\nsecurity videos, there is a large imbalance between the number\\nof foreground and background pixels, which causes BCE to\\nmiss-localize some actoins. A standard approach to remedy\\nthis is to use the Dice loss [30]; however, we ﬁnd that the\\nlarge variation in scale between different foreground objects\\n(actors) leads the network to focus on the larger actions and\\nignore smaller actions.\\nTo this end, we propose a Patch-Dice Loss (PDL),\\nLPDL(y,ˆy) =K∑\\nk=1(\\n1−2∑M\\ni=1pk,i∗ˆpk,i∑M\\ni=1p2\\nk,i+∑M\\ni=1ˆp2\\nk,i+ϵ)\\n(2)\\nwhereKis the number of patches, Mis the number of pixels\\nper patch, and pk,idenotes the probability value assigned to\\npixeliin patchk. This loss splits frames into many local\\nneighborhoods (patches), and computes the dice loss on each\\npatch; this forces the network to segment actions of any size.4239\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 3}, page_content='Algorithm 1 The Tubelet-Merge algorithm which merges\\ntubelets into action-agnostic tubes. The C HECK ENDfunction\\ndetermines if a candidate tube becomes a ﬁnal tube or is merged\\nwith another candidate.\\nInput: A stream of tubelets, S, from the classiﬁer\\nOutput: A set of action-agnostic tubes, Tdone\\nNotation: Inter temp calculates temporal overlap between tubelets.\\n|M[(τc,∗)]|returns the cardinality of the set {τ:M[(τc,τ)]>0}.\\n1:procedure TUBELET -MERGE (S)\\n2:Tprev,Tdone←{} ⊿Initialize candidate and ﬁnal tubes\\n3: M←initialize hash table\\n4: whileτcinS do ⊿Continue until the stream of tubelets ends\\n5: for allτpinTprev do\\n6: ifInter temp(τp,τc)>0then\\n7: M[(τp,τc)]←IoU(τp,τc)\\n8: else\\n9: C HECK END(τp,Tprev ,M)\\n10: append τctoTprev⊿Tubelet becomes a candidate tube\\n11: whileTprev is not empty do⊿Deals with remaining candidates\\n12:τp←Tprev[0]\\n13: C HECK END(τp,Tprev ,M)\\n14: returnTdone\\n1:function CHECK END(τp,Tprev,M)\\n2: if|M[(τp,∗)]|== 0 then\\n3: M OVE(τp,Tprev ,Tdone )⊿MovesτpfromTprev toTdone\\n4: else if|M[(τp,∗)]|== 1 then\\n5:τi←maxτiM[(τp,τi)]\\n6: if|M[(∗,τi)]|== 1 then\\n7: M ERGE (τp,τi,Tprev ,M)\\n8: else\\n9: M OVE(τp,Tprev ,Tdone )\\n10: else\\n11:τi←maxτiM[(τp,τi)]\\n12: M ERGE (τp,τi,Tprev ,M)\\n1:function MERGE (τ1,τ2,Tprev,M)⊿Merges two candidate tubes\\n2:τ1←(f1\\n1,f2\\n2,{b1,b2},{a1,a2}) ⊿{}is concatenation\\n3: remove τ2fromTprev\\n4: M[τ1,τi]←M[τ2,τi]⊿Done for all τiwhere M[τ2,τi]≥0\\nOur ﬁnal loss is a weighted sum of BCE and PDL, calculated\\nover multiple scales:\\nLloc=S∑\\ns=1λ1LBCE(\\ny(s),ˆy(s))\\n+λ2LPDL(\\ny(s),ˆy(s))\\n, (3)\\nwhereSdenote number of layers and for a layer s,y(s)andˆy(s)\\nare the ground-truth and predicted segmentations respectively.\\nTubelet Extraction: The segmentation output for each clip\\nis a foreground-background conﬁdence mask which isolates\\npotential action tubes. To obtain individual tubelets from this\\nsegmentation output, we threshold the output to create a\\nbinary mask followed by spatio-temporal connected component\\nextraction. The connected component [8], [34] process will\\ngenerate tubelets for all spatially and temporally linked pixels.\\nC. Classiﬁcation Network\\nOnce the tubelets are extracted, our system assigns its action\\nlabels. Our classiﬁcation network is an R(2+1)D network [32],\\nwhich generates C+ 1 probability outputs, where Cis the\\nnumber of action classes and the additional output is for the\\nbackground class (used in the case where no action is present in\\nthe tubelet). Since multiple actions could occur simultaneously\\nin one tube, we treat this as a multi-label classiﬁcation problem.Algorithm 2 The Action-Split algorithm which converts the\\naction-agnostic tubes into action-speciﬁc predictions.\\nInput: A set of action-agnostic tubes, T, and a set of actions, C\\nOutput: A set of action-speciﬁc tubes, AG\\nNotation: The hyperparameters κ,α,β, andγare described in the\\nsupplementary materials. ai\\nc[f]andτi[f]contain the action prediction\\nscores and tube information at frame f, respectively.\\n1:procedure ACTION -SPLIT(T)\\n2:AG←{} ⊿Initializes the action-speciﬁc tubes\\n3: for allτiinTdo\\n4:τsmooth←SMOOTH (τi)\\n5: for allcin1 :Cdo ⊿Loop through each action class\\n6: aL←EXTRACT (τsmooth,c)\\n7: append aLtoAG\\n8: returnAG\\n1:function SMOOTH (τi)\\n2: for allfinfi\\n1:fi\\n2do\\n3:ai\\nc[f]←1\\n2κ+1∑κ\\nk=−κai\\nc[f+k]\\n4: returnτi\\n1:function EXTRACT (τi,c) ⊿Extracts tubes of a speciﬁc class\\n2:AL,al←{}⊿Initialize extracted action tubes and a placeholder\\n3:count←0\\n4: for allfinfi\\n1:fi\\n2do\\n5: ifai\\nc[f]>α then ⊿Continue current action tube\\n6: append τi[f]toal\\n7: count←0\\n8: else\\n9: count←count + 1\\n10: ifcount>β then ⊿Current action tube is ﬁnished\\n11: append altoAL\\n12: al←{},count←0\\n13: remove tubes shorter than γfromAL\\n14: returnAL\\nInstead of using a softmax activation for the probability outputs,\\nwhich is common for single-label action classiﬁers, a sigmoid\\nactivation is used which allows multiple actions classiﬁed\\nwithin a single tubelet. To train this classiﬁer, we use a BCE\\nloss (equation 1) summed over all C+ 1probability outputs.\\nD. TMAS Algorithm'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 3}, page_content='activation is used which allows multiple actions classiﬁed\\nwithin a single tubelet. To train this classiﬁer, we use a BCE\\nloss (equation 1) summed over all C+ 1probability outputs.\\nD. TMAS Algorithm\\nTo merge the tubelets and obtain the ﬁnal action detections\\n(tubes), we propose the Tubelet-Merge Action-Split algorithm\\n(TMAS). Each tubelet, denoted τi, is described as follows:(\\nfi\\n1,fi\\n2,bi,ai\\nc)\\nwherefi\\n1is the start time, fi\\n2is the end time, bi\\nare the bounding boxes for each frame of the tubelet, and ai\\ncare\\nthe frame-level action probability c∈{0,1,...C}, where 0is\\nbackground. The TMAS algorithm consists of two steps. First,\\nwe merge the tubelets into action-agnostic tubes of varying\\nlength; then, we split these action-agnostic tubes into a set\\nof action-speciﬁc tubes which contain the localizations for the\\nvarious activities in the video.\\nTubelet-Merge: The procedure to merge tubelets into action-\\nagnostic tubes is described in Algorithm 1. The input to this\\nis a temporally sequential stream of tubelets coming from the\\nclassiﬁcation network. The set of candidate tubes is initialized\\nwith the ﬁrst tubelet. For each subsequent tubelet, we calculate\\nthe spatio-temporal overlap with the existing candidate tubes.\\nThis results in four possible outcomes: 1) if there is no overlap,\\nthe tubelet becomes a new candidate tube, 2) if there is\\nan overlap between a single candidate tube and the tubelet,\\nthey are merged and become a new candidate tube, 3) if4240\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 4}, page_content='the tubelet has an overlap with multiple candidates, then the\\ntubelet becomes a new candidate, 4) if multiple tubelets have\\nan overlap with a single candidate tube, then the tubelet with\\nthe highest overlap is merged with that candidate and the other\\ntubelets become separate candidate tubes. Once all tubelets are\\nchecked, the candidate tubes become the output action-agnostic\\ntubes.\\nAction-Split: From the action-agnostic tubes, we obtain\\naction-speciﬁc detections using the Action-Split procedure de-\\nscribed in Algorithm 2. We start by smoothing out per-frame\\naction conﬁdence scores; which accounts for fragmentation\\ncaused by action miss-classiﬁcations. Then we build the action-\\nspeciﬁc tubes by checking for continuous occurrences of each\\naction class; this allows several occurrences of the same activity\\nto occur within a single tube. For instance, a person walking\\nmight stop and stand for several seconds and start walking\\nagain; this entire sequence will be contained in a single tube,\\nbut the Action-Split procedure will correctly generate two\\nseparate instances of activity walking and one instance of\\nactivity standing . To be robust to classiﬁcation errors, action\\ntubes with the same action label that are within a limited\\ntemporal neighborhood are combined together to form a single\\ncontinuous action prediction.\\nRuntime Complexity: The worst-case runtime of our TMAS\\nalgorithm isO(\\nn2)\\n, wherenis the total number of candidate\\ntubes at any given time. However, we sequentially process\\nour tubelets and constantly shift the candidate tubes which\\ncan not have any possible future match to the set of ﬁnal\\ntubes. Therefore, the set of candidate tubes at any particular\\ntime is reasonably small and our TMAS algorithm contributes\\nnegligible overhead to our system’s overall computation time.\\nIV. E XPERIMENTAL SETUP\\nDatasets: We evaluate our method on two large-scale action\\ndetection datasets with untrimmed security videos: VIRAT\\nand MEV A. The ﬁrst dataset consists of videos from the\\nVIRAT [21] dataset with added action detection annotations.\\nIt contains 64 videos (2.47 hours) for training and 54 videos\\n(1.93 hours) for validation, with annotations for 40 different\\nactivities involving people and vehicles. There is also a held-out\\ntest set containing 246 videos (10.11 hours) whose annotations\\nare not made public. The MEV A dataset [1] consists of 1056\\nannotated videos, each 5 minutes long, covering both indoor\\nand outdoor scenes. We use 936 of these videos for training\\nand of the remaining 120, we use 50 for validation and 70\\nfor local evaluation. These videos are annotated with 37 dif-\\nferent activities, mainly involving humans and vehicles. These\\nannotations follow a long-tail distribution, i.e, there are few\\nactivities which have many annotated instances as they occur\\nvery frequently and there are many activities which have very\\nfew instances as they are rare. For the ﬁnal testing, the system\\nis submitted to an evaluation server where a set of sequestered\\nvideos are used for evaluation of the system. More information\\nabout the sequestered data and testing protocol for MEV A can\\nbe found at ActEv2website.\\n2https://actev.nist.gov/sdlA. Training Details\\nNetwork Training The videos for both datasets are high\\nresolution, so we rescale the videos to a lower resolution of\\n800×448while maintaining the aspect ratio. The localization\\nnetwork uses a stack of 16frames to obtain the binary\\nsegmentation masks; the ground-truth for these masks is the\\nbounding box annotations for all actions present within the\\ngiven frames (regions within the bounding boxes are considered\\nforeground and other regions are considered background). The\\nnetwork is trained using SGD [26], with a learning rate of 1e-\\n3 for about 100000 iterations. For our BCE+PDL training, we\\nhave set both the values of λ1andλ2to1. Our classiﬁcation\\nmodel is trained with a clip length of 16 frames (with a skip\\nrate of 1 to obtain a second long clip) and a spatial-resolution'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 4}, page_content='have set both the values of λ1andλ2to1. Our classiﬁcation\\nmodel is trained with a clip length of 16 frames (with a skip\\nrate of 1 to obtain a second long clip) and a spatial-resolution\\nof112×112. For the classiﬁer, we use the ADAM optimizer\\n[18] with a learning rate of 1e-4 for 75000 iterations on two\\nNVIDIA GeForce Titan X GPUs.\\nData augmentation To increase the diversity of data, we\\npre-process the videos which are input to the network during\\ntraining. For the localization network, we apply frame jitter\\nand cropping to simulate the shaking of a camera which can\\nhappen due to wind. For the classiﬁcation network, we perform\\ncropping, resizing, and horizontal ﬂipping on the input tubes.\\nMoreover, we use both ground-truth and predicted (outputs of\\nthe localization network) tubes for the training of the classiﬁer.\\nOne of the challenging issues with both the VIRAT and\\nMEV A datasets is data imbalance. To balance the data, we ﬁrst\\nunder-sample the classes with a large number of samples. Also,\\nwe perform multi-scale cropping and horizontal ﬂipping on\\nclasses with the fewest number of samples. Lastly, we perform\\nframe reversal to generate new clips for complementary pairs\\nof classes such as ( Opening ,Closing ), (Loading ,Unloading ),\\n(Entering ,Exiting ), and ( Open Trunk ,Close Trunk ) to increase\\nthe number of samples for these classes.\\nMetrics We evaluate the performance of our system using\\nseveral metrics: probability of missed detection at ﬁxed rate of\\nfalse alarm per minute (P miss@R FA), probability of missed de-\\ntection at ﬁxed time-based false alarm per minute (P miss@T FA),\\nand partial area under the Detection Error Tradeoff curve\\n(AUDC). These measure the quality of action detections for the\\naction detection task. To calculate these metrics, a one-to-one\\ncorrespondence is found between the ground-truth actions and\\nthe detected actions; ground-truth actions without a correspond-\\ning detection are missed detections, while detections without\\ncorresponding ground-truth actions are false alarms. For more\\ndetailed explanations of the evaluation metrics as well as the\\nevaluation code, we refer to TRECVID-2019 [2] and MEV A\\nSDL [1]. For ablations of the classiﬁcation network, we use\\nstandard multi-label classiﬁcation metrics: precision, recall, and\\nF1-score.\\nV. R ESULTS\\nIn this section, we present the evaluation of our overall\\nmethod as well as ablations on its individual components.4241\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 5}, page_content='Fig. 3. Runtime vs AUDC Score of different systems on MEV A test set.\\nA. System Evaluation\\nTesting Environment: The proposed system is evaluated\\non the remote test servers provided by NIST. The testing\\nsystem expects a program that is compatible with the ActEV\\nCommand Line Interface (CLI) protocol2. CLI protocol helps\\nour system to communicate with the testing environment and\\nto be executed remotely. It runs all of the submitted algorithms\\nand reports the scores on the publicly available leaderboard.\\nVIRAT: We present our system’s performance on the VI-\\nRAT test set from TRECVID-2019 leaderboard in Table I\\nfor temporal action localization. We outperform every other\\ncompeting team on Pmiss @0.15TFAandPmiss @0.15RFAmet-\\nrics. Compared to method MUDSML (the best performing\\ncomparison) we achieve similar results (within 0.7%) with\\nrespect to AUDC, but we achieve over a 9.5%improvement in\\nterms ofPmiss @0.15RFA.\\nTeam Pmiss @0.15TFA Pmiss @0.15RFA AUDC\\nFraunhofer 0.7747 0.8474 0.8270\\nvireoJD-MM 0.5482 0.7284 0.6012\\nNTT CQUPT 0.5112 0.8725 0.6005\\nHitachi 0.5099 0.8240 0.5988\\nBUPT-MCPRL 0.4328 0.7491 0.5240\\nMUDSML [20] 0.3915 0.7979 0.4840\\nOurs 0.3858 0.7022 0.4909\\nTABLE I\\nTEMPORAL LOCALIZATION RESULTS ON THE VIRAT TEST SET FROM THE\\nTRECVID-2019 LEADERBOARD . ALL THE METRICS RELATE TO THE\\nMISS -RATE ,SO LOWER VALUES INDICATE BETTER PERFORMANCE .\\nMEV A: We present the results of our system on the MEV A\\nsequestered test set in Table II. Our method achieves state-\\nof-the-art results in both metrics: improving AUDC by over\\n3.5% andPmiss @0.04TFAby over 2%. Notably, our system\\noutperforms others without the need of pre-trained object\\ndetectors for localization or optical ﬂow for classiﬁcation.\\nB. Run-time analysis\\nWe compare the speed and performance of our system\\nwith other systems on the MEV A test set in Figure 3. All\\nsystems are tested on 4 NVIDIA RTX 2080 Ti GPUs which is\\nthe standard conﬁguration for the evaluation system [2]. Our\\nonline action detection method outperforms all most all the\\n2https://actev.nist.gov/sdlTeam AUDC Pmiss @0.04TFA Processing Time\\nTeam-Vision 0.717 0.776 0.793\\nIBM-MIT-Purdue 0.641 0.733 0.272\\nEdge-Intelligence 0.628 0.754 0.939\\nINF 0.489 0.559 0.646\\nUMD [9] 0.475 0.544 0.725\\nOurs 0.438 0.523 0.362\\nTABLE II\\nTEMPORAL LOCALIZATION RESULTS ON MEVA SEQUESTERED TEST SET .\\nALL THE METRICS RELATE TO THE MISS -RATE ,SO LOWER VALUES\\nINDICATE BETTER PERFORMANCE . THESE RESULTS ARE FROM THE\\nPUBLICLY AVAILABLE LEADERBOARD2.PmissRFA IS NOT INCLUDED IN\\nTHIS TABLE AS IT IS NOT MADE PUBLIC .\\nModel IoU\\nBCE 62.27%\\nBCE + Dice Loss 62.35%\\nBCE + PDL 63.43%\\nTABLE III\\nABLATION EXPERIMENTS TO STUDY THE EFFECT OF THE PATCH -DICE\\nLOSS ON THE LOCALIZATION NETWORK .\\nother systems by a wide margin. Our method also achieves\\nhigher than real-time speed, 45fps, on a single GPU. This large\\ndifference in speed is mainly due to our localization network:\\nit directly generates tubelets instead of relying on per-frame\\nobject detections for proposal generation. This greatly reduces\\nthe number of action proposals and allows us to process videos\\nonline very efﬁciently.\\nFig. 4. Qualitative results from the localization network overlaid on the input\\nframe; the three rows demonstrate action masks obtained from the ground-\\ntruth and generated using the BCE, and Patch-Dice loss respectively. The ﬁrst\\ntwo columns demonstrate that the network trained with Patch-Dice loss can\\ndetect small actions that are missed or partially detected if BCE loss was used.\\nThe third column demonstrates that the localization masks generated using the\\nPatch-Dice loss have better action boundaries.\\nArchitecture Precision Recall F1-Score\\nI3D [3] 0.36 0.31 0.33\\nP3D [24] 0.43 0.41 0.41\\n3D-ResNet [12] 0.46 0.43 0.44\\nR(2+1)D [32] 0.50 0.43 0.45\\nTABLE IV\\nABLATION EXPERIMENTS FOR DIFFERENT CLASSIFICATION NETWORK\\nARCHITECTURES . PRECISION , RECALL ,AND F1- SCORES ARE AVERAGED\\nOVER ALL CLASSES ON THE VIRAT VALIDATION SET .4242'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 5}, page_content='TABLE IV\\nABLATION EXPERIMENTS FOR DIFFERENT CLASSIFICATION NETWORK\\nARCHITECTURES . PRECISION , RECALL ,AND F1- SCORES ARE AVERAGED\\nOVER ALL CLASSES ON THE VIRAT VALIDATION SET .4242\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 6}, page_content='Fig. 5. Qualitative results of our system on some sample local evaluation videos. Each row is a sample output from our system, showing spatio-temporal\\nlocalization and classiﬁcation of actions in speciﬁc frames of the input video. Each action type is shown with different colored bounding box. The example\\nactivities shown here are vehicle turns left , vehicle reverses , vehicle starts , person talks to person , and person opens vehicle door . These results\\ndemonstrate the ability of our system in handling variation of object scales and detecting multiple action classes.\\nC. Ablations\\nPatch-Dice Loss: We run several ablations to evaluate the\\neffectiveness of the Patch-Dice Loss, and present the results in\\nTable III. Using PDL during training leads to an improvement\\nin the localization network, mainly due to the increase in the\\nnumber of correct detections. Although the regular dice loss\\nimproves localization when compared to standard BCE, we\\nﬁnd that the network does not correctly localize the very small\\nactivities. By using PDL during training, the network correctly\\nlocalizes more of these activities which leads to an overall\\nimprovement in the AUDC score.\\nClassiﬁcation Network: We experiment with multiple clas-\\nsiﬁcation models to determine the best network architecture for\\nour system. For a fair comparison, all models are initialized\\nwith pre-trained weights on the Kinetics [17] and are trained\\nwith the same settings. A comparison of their performance on\\nthe VIRAT validation set is shown in Table IV. We use the\\naverage F1-Score as a metric for comparison and observe that\\nR(2+1)D model [32] outperforms the other models.\\nTMAS System TMAS is the ﬁnal step in our system and\\nis crucial for its success. To show the impact of this step in\\nthe overall performance, we compare per-class n-AuDC scores\\nwith and without the TMAS algorithm on our local evaluation\\nset of the MEV A dataset. Please refer to Figure 6. With the\\npost-processing step, we observe that the scores improve for\\nthe activity classes which occur for a longer temporal span\\nsuch as ‘person reads document’(20) and ‘person texts on\\nphone’(24). Please refer to Kitware page3for activity name\\nand the corresponding indices.\\nD. Qualitative Analysis\\nWe present some qualitative results of our system in Figure\\n5. Our system performs well on different viewpoints, action\\nscales, and action types. We are able to detect activities involv-\\ning multiple actors, as well as activities involving interactions\\nbetween a person and a vehicle. Since we do not rely on frame-\\nbased object detection, we produce fewer detections and avoid\\n3https://tinyurl.com/rum4ykm\\nFig. 6. Per-class n-AuDC scores from our system, with and without the\\nTMAS. The results show the signiﬁcance of the TMAS algorithm in the overall\\nperformance of our system. The labels on the x-axis are the class indices and\\nthe numbers in the brackets indicate the average length of the activity in frames.\\nPlease refer to the Kitware annotations page3for the activity names.\\nobjects which are not involved in an activity - this results in\\na drastic reduction in computing power used to classify non-\\nactions, like stationary vehicles.\\nE. Online action detection:\\nOnline action detection is different from traditional action\\ndetection as its goal is to detect an action as it occurs. Our\\nproposed system is an online action detection system as it\\ncan process a stream of input frames: it performs localization,\\nclassiﬁcation, and temporal segmentation of activities with\\nlittle or no delay. Other systems, such as [10] and [20] are\\nrestricted to ofﬂine detection as they rely on object detection\\nfor every frame in the video, requiring access to future frames\\nto generate tube proposals. While [9] improves computation\\ntime, it relies on optical ﬂow computation and produces many\\nproposals, causing trade-off in system performance. This is a\\nmajor advantage of our system as it can be readily used in\\nreal-world security applications.\\nVI. C ONCLUSION'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 6}, page_content='proposals, causing trade-off in system performance. This is a\\nmajor advantage of our system as it can be readily used in\\nreal-world security applications.\\nVI. C ONCLUSION\\nIn this work, we propose Gabriella, a real-time online\\nsystem to detect activities in untrimmed security videos. The\\nproposed system consists of three main components which4243\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 7}, page_content='include tubelet extraction, classiﬁcation, and online tubelet\\nmerging. The proposed approach processes short video clips\\nindependently which helps in real-time online processing. The\\nefﬁcient merging of tubelets using the TMAS algorithm makes\\nthe action detections robust to varying length activities. In\\ncontrast to existing approaches, it does not require frame-\\nlevel object detection or optical ﬂow extraction which are\\ncomputationally expensive and need externally trained models.\\nThe proposed method provides state-of-the-art results on the\\nVIRAT and MEV A datasets with a processing speed of over\\n100 fps.\\nACKNOWLEDGEMENTS\\nThis research is based upon work supported by the Ofﬁce\\nof the Director of National Intelligence (ODNI), Intelligence\\nAdvanced Research Projects Activity (IARPA), via IARPA\\nR&D Contract No. D17PC00345. The views and conclusions\\ncontained herein are those of the authors and should not\\nbe interpreted as necessarily representing the ofﬁcial policies\\nor endorsements, either expressed or implied, of the ODNI,\\nIARPA, or the U.S. Government. The U.S. Government is au-\\nthorized to reproduce and distribute reprints for Governmental\\npurposes notwithstanding any copyright annotation thereon.\\nREFERENCES\\n[1] Kitware inc, the multiview extended video with activities (meva) dataset.\\n[2] Trecvid 2019 actev: Activities in extended video.\\n[3] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new\\nmodel and the kinetics dataset. In proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition , pages 6299–6308, 2017.\\n[4] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking\\natrous convolution for semantic image segmentation. arXiv preprint\\narXiv:1706.05587 , 2017.\\n[5] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-\\ndecoder with atrous separable convolution for semantic image segmen-\\ntation. In Proceedings of the European conference on computer vision\\n(ECCV) , pages 801–818, 2018.\\n[6] K. Duarte, Y . Rawat, and M. Shah. Videocapsulenet: A simpliﬁed\\nnetwork for action detection. In Advances in Neural Information\\nProcessing Systems , pages 7610–7619, 2018.\\n[7] C. Feichtenhofer, H. Fan, J. Malik, and K. He. Slowfast networks for\\nvideo recognition, 2018.\\n[8] C. Fiorio and J. Gustedt. Two Linear Time Union-Find Strategies for\\nImage Processing. Theoretical Computer Science , 154(2), 1996.\\n[9] J. Gleason, C. D. Castillo, and R. Chellappa. Real-time detection of\\nactivities in untrimmed videos. In Proceedings of the IEEE Winter\\nConference on Applications of Computer Vision Workshops , 2020.\\n[10] J. Gleason, R. Ranjan, S. Schwarcz, C. Castillo, J.-C. Chen, and R. Chel-\\nlappa. A proposal-based solution to spatio-temporal action detection in\\nuntrimmed videos. In 2019 IEEE Winter Conference on Applications of\\nComputer Vision (WACV) . IEEE, 2019.\\n[11] C. Gu, C. Sun, D. A. Ross, C. V ondrick, C. Pantofaru, Y . Li, S. Vi-\\njayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, et al. Ava: A\\nvideo dataset of spatio-temporally localized atomic visual actions. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 6047–6056, 2018.\\n[12] K. Hara, H. Kataoka, and Y . Satoh. Can spatiotemporal 3d cnns retrace\\nthe history of 2d cnns and imagenet? In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) , 2018.\\n[13] R. Hou, C. Chen, and M. Shah. An end-to-end 3d convolutional neural\\nnetwork for action detection and segmentation in videos. arXiv preprint\\narXiv:1712.01111 , 2017.\\n[14] H. Idrees, A. R. Zamir, Y .-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar,\\nand M. Shah. The thumos challenge on action recognition for videos “in\\nthe wild”. Computer Vision and Image Understanding , 155:1–23, 2017.\\n[15] V . Kalogeiton, P. Weinzaepfel, V . Ferrari, and C. Schmid. Action tubelet\\ndetector for spatio-temporal action localization. In Proceedings of the\\nIEEE International Conference on Computer Vision , pages 4405–4413,'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 7}, page_content='detector for spatio-temporal action localization. In Proceedings of the\\nIEEE International Conference on Computer Vision , pages 4405–4413,\\n2017.[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-\\nFei. Large-scale video classiﬁcation with convolutional neural networks.\\nInProceedings of the 2014 IEEE Conference on Computer Vision and\\nPattern Recognition , CVPR ’14, pages 1725–1732, Washington, DC,\\nUSA, 2014. IEEE Computer Society.\\n[17] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\\nnarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al. The kinetics\\nhuman action video dataset. arXiv preprint arXiv:1705.06950 , 2017.\\n[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\\narXiv preprint arXiv:1412.6980 , 2014.\\n[19] A. Kirillov, R. Girshick, K. He, and P. Doll ´ar. Panoptic feature pyramid\\nnetworks. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , pages 6399–6408, 2019.\\n[20] W. Liu, G. Kang, P.-Y . Huang, X. Chang, Y . Qian, J. Liang, L. Gui,\\nJ. Wen, and P. Chen. Argus: Efﬁcient activity detection system for\\nextended video analysis. In Proceedings of the IEEE Winter Conference\\non Applications of Computer Vision Workshops , pages 126–133, 2020.\\n[21] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee,\\nS. Mukherjee, J. Aggarwal, H. Lee, L. Davis, et al. A large-scale\\nbenchmark dataset for event recognition in surveillance video. In CVPR\\n2011 , pages 3153–3160. IEEE, 2011.\\n[22] X. Peng and C. Schmid. Multi-region two-stream r-cnn for action\\ndetection. In European conference on computer vision , pages 744–759.\\nSpringer, 2016.\\n[23] A. J. Piergiovanni and M. S. Ryoo. Temporal gaussian mixture layer for\\nvideos. In Proceedings of the 36th International Conference on Machine\\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,\\npages 5152–5161, 2019.\\n[24] Z. Qiu, T. Yao, and T. Mei. Learning spatio-temporal representation with\\npseudo-3d residual networks. In proceedings of the IEEE International\\nConference on Computer Vision , pages 5533–5541, 2017.\\n[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks. In Advances in neural\\ninformation processing systems , pages 91–99, 2015.\\n[26] H. Robbins and S. Monro. A stochastic approximation method. The\\nannals of mathematical statistics , pages 400–407, 1951.\\n[27] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional networks\\nfor biomedical image segmentation. In Medical Image Computing and\\nComputer-Assisted Intervention (MICCAI) , volume 9351 of LNCS , pages\\n234–241. Springer, 2015.\\n[28] K. Simonyan and A. Zisserman. Two-stream convolutional networks for\\naction recognition in videos. In Z. Ghahramani, M. Welling, C. Cortes,\\nN. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural\\nInformation Processing Systems 27 , pages 568–576. Curran Associates,\\nInc., 2014.\\n[29] G. Singh, S. Saha, M. Sapienza, P. H. Torr, and F. Cuzzolin. Online\\nreal-time multiple spatiotemporal action localisation and prediction. In\\nProceedings of the IEEE International Conference on Computer Vision ,\\npages 3637–3646, 2017.\\n[30] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. J. Cardoso.\\nGeneralised dice overlap as a deep learning loss function for highly\\nunbalanced segmentations. In Deep learning in medical image analysis\\nand multimodal learning for clinical decision support , pages 240–248.\\nSpringer, 2017.\\n[31] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning\\nspatiotemporal features with 3d convolutional networks. In The IEEE\\nInternational Conference on Computer Vision (ICCV) , December 2015.\\n[32] D. Tran, H. Wang, L. Torresani, J. Ray, Y . LeCun, and M. Paluri. A\\ncloser look at spatiotemporal convolutions for action recognition. In\\nProceedings of the IEEE conference on Computer Vision and Pattern\\nRecognition , pages 6450–6459, 2018.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 7}, page_content='closer look at spatiotemporal convolutions for action recognition. In\\nProceedings of the IEEE conference on Computer Vision and Pattern\\nRecognition , pages 6450–6459, 2018.\\n[33] S. Vyas, Y . S. Rawat, and M. Shah. Multi-view action recognition using\\ncross-view video prediction. In Proceedings of the European Conference\\non Computer Vision , 2020.\\n[34] K. Wu, E. Otoo, and A. Shoshani. Optimizing connected component\\nlabeling algorithms. In J. M. Fitzpatrick and J. M. Reinhardt, editors,\\nMedical Imaging 2005: Image Processing , volume 5747, pages 1965 –\\n1976. International Society for Optics and Photonics, SPIE, 2005.\\n[35] Z. Yang, J. Gao, and R. Nevatia. Spatio-temporal action detec-\\ntion with cascade proposal and location anticipation. arXiv preprint\\narXiv:1708.00042 , 2017.\\n[36] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-\\nFei. Every moment counts: Dense detailed labeling of actions in complex\\nvideos. International Journal of Computer Vision , 2017.4244\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader= PyPDFLoader(\"rizve2021.pdf\")\n",
    "pages= loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the questions based on the content below. If you cannot answer the question, reply with \"I'm not sure of the anser\" \n",
      "Context: Here is the context\n",
      "Question: Here is the question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template= \"\"\"\n",
    "Answer the questions based on the content below. If you cannot answer the question, reply with \"I'm not sure of the anser\" \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt= PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is the context\", question=\"Here is the question\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course! Here are the answers to your questions based on the provided context:\\n\\n1. Who was Ram and what did he do?\\nRam is the name of the farmer who grew barley in his fields. Therefore, Ram is a farmer who grows barley.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= prompt| model\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"There lived a farmer Ram who grew Barley in his fields\",\n",
    "        \"question\": \"Who was Ram and what did he do?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create embedding of the pages and store them in a vectorDB so that we can pass it in the \"context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "vectorstore= DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rizve2021.pdf', 'page': 5}, page_content='Fig. 3. Runtime vs AUDC Score of different systems on MEV A test set.\\nA. System Evaluation\\nTesting Environment: The proposed system is evaluated\\non the remote test servers provided by NIST. The testing\\nsystem expects a program that is compatible with the ActEV\\nCommand Line Interface (CLI) protocol2. CLI protocol helps\\nour system to communicate with the testing environment and\\nto be executed remotely. It runs all of the submitted algorithms\\nand reports the scores on the publicly available leaderboard.\\nVIRAT: We present our system’s performance on the VI-\\nRAT test set from TRECVID-2019 leaderboard in Table I\\nfor temporal action localization. We outperform every other\\ncompeting team on Pmiss @0.15TFAandPmiss @0.15RFAmet-\\nrics. Compared to method MUDSML (the best performing\\ncomparison) we achieve similar results (within 0.7%) with\\nrespect to AUDC, but we achieve over a 9.5%improvement in\\nterms ofPmiss @0.15RFA.\\nTeam Pmiss @0.15TFA Pmiss @0.15RFA AUDC\\nFraunhofer 0.7747 0.8474 0.8270\\nvireoJD-MM 0.5482 0.7284 0.6012\\nNTT CQUPT 0.5112 0.8725 0.6005\\nHitachi 0.5099 0.8240 0.5988\\nBUPT-MCPRL 0.4328 0.7491 0.5240\\nMUDSML [20] 0.3915 0.7979 0.4840\\nOurs 0.3858 0.7022 0.4909\\nTABLE I\\nTEMPORAL LOCALIZATION RESULTS ON THE VIRAT TEST SET FROM THE\\nTRECVID-2019 LEADERBOARD . ALL THE METRICS RELATE TO THE\\nMISS -RATE ,SO LOWER VALUES INDICATE BETTER PERFORMANCE .\\nMEV A: We present the results of our system on the MEV A\\nsequestered test set in Table II. Our method achieves state-\\nof-the-art results in both metrics: improving AUDC by over\\n3.5% andPmiss @0.04TFAby over 2%. Notably, our system\\noutperforms others without the need of pre-trained object\\ndetectors for localization or optical ﬂow for classiﬁcation.\\nB. Run-time analysis\\nWe compare the speed and performance of our system\\nwith other systems on the MEV A test set in Figure 3. All\\nsystems are tested on 4 NVIDIA RTX 2080 Ti GPUs which is\\nthe standard conﬁguration for the evaluation system [2]. Our\\nonline action detection method outperforms all most all the\\n2https://actev.nist.gov/sdlTeam AUDC Pmiss @0.04TFA Processing Time\\nTeam-Vision 0.717 0.776 0.793\\nIBM-MIT-Purdue 0.641 0.733 0.272\\nEdge-Intelligence 0.628 0.754 0.939\\nINF 0.489 0.559 0.646\\nUMD [9] 0.475 0.544 0.725\\nOurs 0.438 0.523 0.362\\nTABLE II\\nTEMPORAL LOCALIZATION RESULTS ON MEVA SEQUESTERED TEST SET .\\nALL THE METRICS RELATE TO THE MISS -RATE ,SO LOWER VALUES\\nINDICATE BETTER PERFORMANCE . THESE RESULTS ARE FROM THE\\nPUBLICLY AVAILABLE LEADERBOARD2.PmissRFA IS NOT INCLUDED IN\\nTHIS TABLE AS IT IS NOT MADE PUBLIC .\\nModel IoU\\nBCE 62.27%\\nBCE + Dice Loss 62.35%\\nBCE + PDL 63.43%\\nTABLE III\\nABLATION EXPERIMENTS TO STUDY THE EFFECT OF THE PATCH -DICE\\nLOSS ON THE LOCALIZATION NETWORK .\\nother systems by a wide margin. Our method also achieves\\nhigher than real-time speed, 45fps, on a single GPU. This large\\ndifference in speed is mainly due to our localization network:\\nit directly generates tubelets instead of relying on per-frame\\nobject detections for proposal generation. This greatly reduces\\nthe number of action proposals and allows us to process videos\\nonline very efﬁciently.\\nFig. 4. Qualitative results from the localization network overlaid on the input\\nframe; the three rows demonstrate action masks obtained from the ground-\\ntruth and generated using the BCE, and Patch-Dice loss respectively. The ﬁrst\\ntwo columns demonstrate that the network trained with Patch-Dice loss can\\ndetect small actions that are missed or partially detected if BCE loss was used.\\nThe third column demonstrates that the localization masks generated using the\\nPatch-Dice loss have better action boundaries.\\nArchitecture Precision Recall F1-Score\\nI3D [3] 0.36 0.31 0.33\\nP3D [24] 0.43 0.41 0.41\\n3D-ResNet [12] 0.46 0.43 0.44\\nR(2+1)D [32] 0.50 0.43 0.45\\nTABLE IV\\nABLATION EXPERIMENTS FOR DIFFERENT CLASSIFICATION NETWORK\\nARCHITECTURES . PRECISION , RECALL ,AND F1- SCORES ARE AVERAGED\\nOVER ALL CLASSES ON THE VIRAT VALIDATION SET .4242'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 4}, page_content='have set both the values of λ1andλ2to1. Our classiﬁcation\\nmodel is trained with a clip length of 16 frames (with a skip\\nrate of 1 to obtain a second long clip) and a spatial-resolution\\nof112×112. For the classiﬁer, we use the ADAM optimizer\\n[18] with a learning rate of 1e-4 for 75000 iterations on two\\nNVIDIA GeForce Titan X GPUs.\\nData augmentation To increase the diversity of data, we\\npre-process the videos which are input to the network during\\ntraining. For the localization network, we apply frame jitter\\nand cropping to simulate the shaking of a camera which can\\nhappen due to wind. For the classiﬁcation network, we perform\\ncropping, resizing, and horizontal ﬂipping on the input tubes.\\nMoreover, we use both ground-truth and predicted (outputs of\\nthe localization network) tubes for the training of the classiﬁer.\\nOne of the challenging issues with both the VIRAT and\\nMEV A datasets is data imbalance. To balance the data, we ﬁrst\\nunder-sample the classes with a large number of samples. Also,\\nwe perform multi-scale cropping and horizontal ﬂipping on\\nclasses with the fewest number of samples. Lastly, we perform\\nframe reversal to generate new clips for complementary pairs\\nof classes such as ( Opening ,Closing ), (Loading ,Unloading ),\\n(Entering ,Exiting ), and ( Open Trunk ,Close Trunk ) to increase\\nthe number of samples for these classes.\\nMetrics We evaluate the performance of our system using\\nseveral metrics: probability of missed detection at ﬁxed rate of\\nfalse alarm per minute (P miss@R FA), probability of missed de-\\ntection at ﬁxed time-based false alarm per minute (P miss@T FA),\\nand partial area under the Detection Error Tradeoff curve\\n(AUDC). These measure the quality of action detections for the\\naction detection task. To calculate these metrics, a one-to-one\\ncorrespondence is found between the ground-truth actions and\\nthe detected actions; ground-truth actions without a correspond-\\ning detection are missed detections, while detections without\\ncorresponding ground-truth actions are false alarms. For more\\ndetailed explanations of the evaluation metrics as well as the\\nevaluation code, we refer to TRECVID-2019 [2] and MEV A\\nSDL [1]. For ablations of the classiﬁcation network, we use\\nstandard multi-label classiﬁcation metrics: precision, recall, and\\nF1-score.\\nV. R ESULTS\\nIn this section, we present the evaluation of our overall\\nmethod as well as ablations on its individual components.4241\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 0}, page_content='the center frame. Untrimmed action detection datasets, like\\nTHUMOS’14 [14], are comprised of untrimmed videos, but\\neach video contains only one or two actors performing the\\nsame action. Although difﬁcult, these datasets do not contain\\nall the aforementioned challenges, which results in action\\ndetection methods that perform poorly when evaluated on\\nvideos containing all these challenges. Therefore, in this work\\nwe focus on the security video datasets: VIRAT [21] and\\nMEV A (Multiview Extended Video with Activities) [1]. Not\\nonly do these two action detection datasets have untrimmed\\nvideos with multiple activity types and multiple actors, but\\nalso they are comprised of multiple viewpoints and contain\\nseveral actors performing multiple actions concurrently . These\\nactors have varying scales and actor sizes tend to be extremely\\nsmall relative to the video frame, which makes the detection of\\nactivities in these datasets extremely difﬁcult. Figure 1 shows\\nsample frames from the VIRAT dataset and compares them\\nwith frames from the THUMOS’14 and A V A datasets.\\nWe focus on untrimmed security videos and propose\\nGabriella , an online real-time system for action detection. Our2020 25th International Conference on Pattern Recognition (ICPR)\\nMilan, Italy, Jan 10-15, 2021\\n978-1-7281-8808-9/20/$31.00 ©2020 IEEE 42372020 25th International Conference on Pattern Recognition (ICPR) | 978-1-7281-8808-9/21/$31.00 ©2021 IEEE | DOI: 10.1109/ICPR48806.2021.9412791\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'rizve2021.pdf', 'page': 1}, page_content='II. R ELATED WORKS\\nConvolutional Neural Networks (CNN) have been studied\\nfor video analysis and applied successfully for the action\\nrecognition problem [3], [31], [33]. Earlier approaches fuse\\n2D frame features to extract temporal information [16], while\\nrecent works mostly apply 3D convolutions to extract spatio-\\ntemporal features simultaneously [3], [7], [31]. The works in\\n[7], [28] use two-stream network architectures to further exploit\\ntemporal dependencies.\\nMost action classiﬁers expect short trimmed videos, however,\\nthis is unrealistic for action recognition in real-world security\\nvideos. Predicting the temporal extents of actions is necessary\\nfor reliable recognition. In [23], a new layer is proposed to\\ntemporally localize activities in videos of the MultiThumos\\ndataset [36]. Most works on spatial action detection rely on\\na region proposal network [25] to detect multiple objects in\\neach frame and combine them temporally to generate action\\ntubelets [22], [35]. In [13], a 3D CNN network efﬁciently\\npredicts frame-wise background-foreground segmentation map\\nand extrapolates the action tubes. However, such approaches\\nbecome computationally inefﬁcient as the number of proposals\\ngrows larger, making it unsuitable for real time performance.\\nAction detection in untrimmed security videos requires to\\naddress multiple challenges. In [10], a frame-level object\\ndetection and optical ﬂow based model is proposed to solve\\naction detection on the VIRAT [21] dataset. They use hi-\\nerarchical clustering on all detected objects in a video to\\nobtain tube proposals and use optical ﬂow to perform action\\nclassiﬁcation. In [20] authors also perform frame-level object\\ndetection followed by tracking to generate proposals. These\\napproaches are computationally expensive and not suitable for\\nonline processing. Recent work by [9] improves upon [10]\\nto make the system real-time by reducing cluster points per\\nvideo, subsampling rate, and use GPU accelerated optical\\nﬂow computation. However, this approach produces too many\\nproposals which ends up affecting the system performance.\\nOur method uses a 3D CNN network for spatio-temporal ac-\\ntion segmentation which produces temporally consistent predic-\\ntions with fewer proposals. Additionally, our system processes\\nvideos in an online fashion without using computationally\\nexpensive methods (region proposal network, tracking, and\\noptical ﬂow) and achieves better performance in real-time.\\nIII. M ETHODOLOGY\\nA. Overview\\nAn overview of our system can be found in Figure 2.\\nEach untrimmed video is ﬁrst split into video clips, which4238\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 17,2021 at 03:12:27 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "retriever.invoke(\"MEVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The challenges of action detection in untrimmed videos include:\\n\\n1. Lack of temporal context: Since untrimmed videos are not edited or trimmed, there is no predefined temporal structure or rhythm to the video. This makes it more difficult for the algorithm to identify and extract actions from the video.\\n2. Variability in actor size: In security videos, actors can appear at different scales, making it challenging for the algorithm to identify and track them properly.\\n3. Occlusion and cluttered scenes: Security videos often contain multiple actors, objects, and background elements that can occlude or clutter the scene, making it difficult for the algorithm to identify actions and segment them from the rest of the video.\\n4. Limited context: Unlike trimmed videos, untrimmed videos do not provide any predefined context or structure. This limits the algorithm's ability to use prior knowledge or assumptions about the video content.\\n5. Real-time processing: The algorithm must be able to process the video in real-time, which adds additional complexity and computational requirements.\\n6. Imbalanced classes: Since security videos often contain a mix of different actions, there can be an imbalance between the number of foreground and background pixels. This can lead to poor performance if not handled properly.\\n7. Limited annotated data: Obtaining large annotated datasets for action detection in security videos can be challenging due to the sensitive nature of the content and the difficulty in obtaining high-quality annotations.\\n8. Complexity of real-world scenarios: Security videos often depict complex and dynamic scenarios, with multiple actors, objects, and background elements that can interact with each other. This complexity can make it difficult for the algorithm to accurately detect and segment actions.\\n9. Noise and artifacts: Untrimmed videos can contain various types of noise and artifacts, such as camera shake, motion blur, and compression artifacts, which can affect the performance of the action detection algorithm.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What are the challenges of action detection in untrimmed videos? \"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
